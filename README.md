# NEAR AI Analytics

A collection of tools for benchmarking, evaluating, and analyzing agent performance metrics.

## Repository Structure

- [`/benchmarks`](./benchmarks/): Tools for running benchmarks on AI models and agents
- [`/canonical_metrics`](./canonical_metrics/): Standard formats and tools for metrics collection
- [`/evaluation`](./evaluation/): Dashboard and tools for evaluating agent or model performance
- [`/historic_performance`](./historic_performance/): Dashboard for tracking performance over time

## Primary Use Cases

### 1. Collect Metrics from Agent Runs

Collect stats from agent runs and convert to canonical metrics format.

### 2. Run Benchmarks and Evaluations

Execute popular and user-owned benchmarks.

### 3. Run Evaluation Dashboard

Visualize, analyze, compare agent & model performances

### 4. Run Historic Performance Dashboard

Track agent performance over time.

## Contributing

We welcome contributions!
